---
title: "Repeated Measures"
author: "Kristian Brock"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = TRUE)

# Any setup R code can go here
library(dplyr)
library(bootcamp)
```


## Welcome

This tutorial introduces some simple model-based approaches for analsying the kind of repeated measures data we generally encounter in clinical trials.

### Learning Objectives
Upon completion of this session, you will be able to:

* fit a random intercepts hierarchical model;
* fit a random gradients hierarchical model.

## Repeated measures data in clinical trials

Repeated measures data arise in a number of ways in clinical trials:

* tumour size assessments, e.g. RECIST
* biological and biochemical variables, e.g. albumin, neutrophils
* quality-of-life measures, e.g. EQ5D

The measurements tend to be continuous; indeed each of the examples above is continuous.
However, that need not be a constraint.
You could very well encounter repeated measures of binary, ordinal and count variables.
The methods introduced in this session will generalise to those circumstances, but the details lay outside the scope of today's material.

## Hierarchical models

There are many methods for analysing repeated measures data.
A good textbook is _Analysis of Longitudinal Data_ by Diggle, Liang and Zeger.

In this session, we will focus on hierarchical models.
These are also sometimes called:

* mixed effects models
* random effects models
* multi-level models

The first two labels have been criticised by Gelman & Hill (and others) because they are ambiguous.
We will try to use the names _hierarchical_ or _multi-level_ models.

These approaches are so-called because they specify:

* a model for the data
* a model for the parameters

### Model for the data
For instance, you would have encountered at university models like:

$$ y_i = \alpha + a_i + \beta x + \epsilon_i, \quad \epsilon \sim N(0, \sigma^2)$$

where $i$ indexes study subjects.
This statement specifies the model for the outcome data, $y$, conditional on parameters and covariates.

We see that the expected value for $y$ takes an element, $\alpha$, common to all subjects.
This is referred to as the _population-level intercept_.

The expected value for $y$ also contains a further element, $a_i$.
These are specific to each subject, hence the $i$ subscript, and are referred to as the _group-level intercepts_.
Strictly speaking, they are the group-level intercepts _at the subject level_ because there can be other group-level effects.

Lastly, the expected value for $y$ contains an element that scales the covariate $x$ by an amount $\beta$.
This is referred to as the _population-level gradient_, with the fact that it pertains to the $x$-variable being understood implicitly but frequently unvoiced.

In nomenclature we do not encourage, you will see $\alpha$ and $\beta$ referred to as _fixed effects_, and $a_i$ referred to as _random effects_.
The above terms, population-level and group-level effects, are preferable.


### Model for the parameters

The above model specification is incomplete because it makes no statement about the parameters, $a_i$.

A typical model in this scenario might be:

$$a_i \sim N(0, \sigma_a^2)$$

It is appropriate to fix the expected value of $a_i$ to be zero because the population-level covariate-invariant mean effect is handled by $\alpha$.
Put another way, if the expected value of $a_i$ was non-zero, the value could be subsumed into $\alpha$.

It is the provision of a model for the parameters that warrants the descriptors _hierarchical_ or _multi-level_.

## Hierarchical models in R

There are several R packages that will fit hierarchical models.

Using a frequentist approach, the two most noteworthy packages are `nlme` and `lme4`.
`nlme` is older.
It fits models with a continuous response variable using the identity link function assuming gaussian errors.
`lme4` also fits models for continuous responses.
It adds support for binary outcomes using the logit link, and count variables using the log link.

Using a Bayesian approach, the packages `rstanarm` and `brms` fit hierarchcial models of all response types via Stan, an incredibly powerful and flexible MCMC library.
Stan will be the focus of a later bootcamp session.

This is not an exhaustive list.



## Model fitting
In this section, we fit various models to the `tumours` dataset in the `bootcamp` package.
The dataset contains repeated measures within patient of tumour-sizes in a two arm RCT.
Initially we will ignore treatment arm.

To refresh your memory, the data look like this:
```{r}
library(bootcamp)
library(dplyr)
library(ggplot2)

tumours %>% 
  ggplot(aes(x = Time, y = TumourSize)) + 
  geom_point() + 
  geom_line(aes(group = TNO), alpha = 0.1) -> p

p
```

We see that tumour sizes are between 3 and 7 cm at baseline, and that they generally shrink during treatment.


### Linear model

Before fitting an hierarchical model, we fit a standard ordinary least squares regression:

```{r, fig.width = 10}
lm0 <- lm(TumourSize ~ 1 + Time, data = tumours)
summary(lm0)
```

The average tumour is about 5cm at baseline and shrinks by about 1mm per month of follow-up.

The R library `broom` _cleans up_ modelling in R by providing functions that summarise and expand the different types of models with standard functions and output.
For instance, the `augment` function produces a table of the data underlying the model, with added predictions, residuals, and SEs:

```{r}
library(broom)
augment(lm0)
```

We can overlay the regression sizes predicted by this model:
```{r}
p + geom_line(
  aes(y = .fitted), 
  data = augment(lm0) %>% distinct(Time, .fitted)
)
```

Note that we provided a new dataset for the new layer in the ggplot object by specifying `data = ` in the call to `geom_line`.
Without this, `ggplot2` would have looked in the original `tumours` dataset for `.fitted` and complained when it did not find it.

We see that the same prediction is made irrespective the baseline value.
Surely we can improve upon this.


### Random intercepts models

The random intercepts model was specified in a previous section:

$$ y_i = (\alpha + a_i) + \beta x + \epsilon_i, \quad \epsilon \sim N(0, \sigma^2)$$

$$a_i \sim N(0, \sigma_a^2)$$

We can fit the mdoel using the `lme` function in the `nlme` package:
```{r}
library(nlme)

lme0 <- lme(fixed = TumourSize ~ 1 + Time, 
            random = ~ 1 | TNO, 
            data = tumours)
summary(lme0)
```

The `fixed` formula says "I want a population coefficient for the effects of the intercept and `Time`".
The `random` formula says "Each TNO will have their own adjustment to the intercept".

We can access the populaiton-level (aka "fixed") effects:
```{r}
fixef(lme0)
```

and the group-level (aka "random") effects:

```{r}
ranef(lme0)
```

As before, `augment` will give us a tbale of predictions based on the underlying data:
```{r}
augment(lme0)
```

and we can add those lines to the plot of the data:
```{r, fig.width = 9, fig.height = 10}
p + 
  geom_line(aes(y = .fitted, group = TNO), data = augment(lme0), col = 'red') +
  facet_wrap(~ TNO, ncol = 10)
```

I have facet-wrapped the plots to make clear how the predictions from this model fit at a patient-level.

Notice that, by design, the intercepts vary but the gradients do not: progression is assumed to be the same with no heterogeneity across the patients.
Look at patients 6 and 7, for instance.
The common gradient is a poor assumption here.

Perhaps we can improve on this model too?


### Random gradients models

The random gradients model takes the random intercepts model and adds group-level gradients:

$$ y_i = (\alpha + a_i) + (\beta + b_i) \, x + \epsilon_i, \quad \epsilon \sim N(0, \sigma^2)$$

$$a_i \sim N(0, \sigma_a^2), \quad b_i \sim N(0, \sigma_b^2)$$

```{r}
lme1 <- lme(fixed = TumourSize ~ 1 + Time, 
            random = ~ 1 + Time | TNO, 
            data = tumours)
summary(lme1)
```

Regard now that there are intercept and gradients in the "random" effects matrix:
```{r}
ranef(lme1)
```

Once again, `augment` returns predictions:
```{r}
augment(lme1)
```

and we can view them in light of the observed data:
```{r, fig.width = 9, fig.height = 10}
p + 
  geom_line(aes(y = .fitted, group = TNO), data = augment(lme1), col = 'red') +
  facet_wrap(~ TNO)
```

Notice how much improved the predictions are for patients 6 and 7.
That should persuade you that the random gradients model is an improvement on the random intercepts model with this dataset.

However, if you like hypothesis tests, you might want a $p$-value.
`lme1` is nested within `lme0`, in that `lme0` is a special case of `lme1` with all $b_i$ set to zero.
We can exploit this to run a likelihood ratio test via the `anova` command in R:

```{r}
anova(lme0, lme1)
```

The $p$-value of the test is microscopic, wholesomely rejecting the hypothesis that the variablility of the $b_i$ terms is zero.


## Exercises

### Exercise 1 - Using `lme4`

The `lme4` package will fit those models too via the `lmer` funciton.
It uses a nicer syntax in that does not split the model formula into two parts but combines the "fixed" and "random" effects into one formula, like

> `y ~ 1 + Time + (1 | TNO)`

See if you can fit the random intercepts and random gradients models in `lme4`.
Are the parameter estimates similar?
How does `augment` behave on a model fit by `lmer`?

### Exercise 2 - Adding a covariate for `Arm`

Using either `nlme` or `lme4`, fit a random gradients model that includes a treatment arm covariate.
Produce a grid of the fitted series with colour denoting `Arm`.
Does progression vary across arms?
By how much, on average?
